# Event-Driven Composition Architecture - Technical Design

## ðŸŽ¯ Architecture Overview

This document defines the technical implementation of the event-driven composition architecture designed in the [CMS + E-commerce Composition Plan](./cms-ecommerce-composition-plan.md). This architecture enables fault-tolerant integration of CMS and E-commerce providers while maintaining our established patterns and reducing implementation complexity from 8.5/10 to 6.5/10.

**Reference Document**: [CMS + E-commerce Composition Plan](./cms-ecommerce-composition-plan.md)
**Architecture Status**: Moving from Phase 1 (Foundation) to Phase 2 (Event-Driven Composition)
**Foundation Status**: 100% CMS coverage (4/4), 75% E-commerce coverage (3/4), Ready for composition

**Design Principles**:
- **Event-Driven Decoupling**: No direct system integration between providers
- **Fault Isolation**: Component failures don't cascade across systems
- **Pluggable Components**: Standard protocol for unlimited provider expansion
- **Unified Content Model**: Single content schema for all SSG engines
- **AWS Serverless**: Scalable, cost-effective, battle-tested patterns

## ðŸ—ï¸ Core Architecture Components

### Event-Driven Integration Flow

```
External Systems          Integration Layer              Build Pipeline
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CMS Provider    â”‚â”€â”€â”€â”€â–¶ â”‚  Integration API       â”‚â”€â”€â”€â”€â–¶â”‚  SNS Event       â”‚
â”‚ (Decap/Sanity/  â”‚      â”‚  (Webhook Endpoints)   â”‚     â”‚  "content.changed"â”‚
â”‚  Tina/Contentful)â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚                               â”‚
                                   â”‚                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ E-commerce      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  Build Trigger   â”‚
â”‚ Provider        â”‚                                     â”‚  Lambda          â”‚
â”‚ (Snipcart/Foxy/ â”‚                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  Shopify)       â”‚                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â–¼
         â”‚                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  CodeBuild       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Unified Content       â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Static Site     â”‚
                      â”‚  Cache (DynamoDB)      â”‚         â”‚  Generation      â”‚
                      â”‚  (Normalized Schema)   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Architecture

```python
# Core integration layer components
class EventDrivenIntegrationLayer:
    """
    Central integration hub handling all provider communications
    through standardized event-driven patterns
    """

    def __init__(self, client_config: ClientConfig):
        # Event bus for decoupled communication
        self.content_events_topic = self._create_sns_topic()

        # Unified content storage with TTL and indexing
        self.unified_content_cache = self._create_dynamodb_table()

        # Integration API handling all external webhooks
        self.integration_api = self._create_api_gateway()

        # Content normalization and event publishing
        self.integration_handler = self._create_integration_lambda()

        # Event-driven build triggering
        self.build_trigger = self._create_build_trigger_lambda()

        # Monitoring and alerting
        self.monitoring_dashboard = self._create_monitoring_system()
```

## ðŸ“Š Unified Content Schema Design

### Core Content Model

```python
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List, Literal
from datetime import datetime
from enum import Enum

class ContentType(str, Enum):
    """Content types supported across all providers"""
    PRODUCT = "product"           # E-commerce product
    ARTICLE = "article"           # Blog post, news article
    PAGE = "page"                 # Static page content
    COLLECTION = "collection"     # Product collection, content category
    MEDIA = "media"               # Images, videos, documents

class ContentStatus(str, Enum):
    """Content lifecycle status"""
    DRAFT = "draft"
    PUBLISHED = "published"
    ARCHIVED = "archived"
    DELETED = "deleted"

class UnifiedContent(BaseModel):
    """
    Unified content schema normalizing data from all CMS and E-commerce providers
    This schema enables SSG engines to work with any provider combination
    """
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True,
        json_schema_extra={
            "examples": [{
                "id": "gid://shopify/Product/123456789",
                "title": "Premium T-Shirt",
                "slug": "premium-t-shirt",
                "content_type": "product",
                "status": "published",
                "provider_type": "ecommerce",
                "provider_name": "shopify_basic"
            }]
        }
    )

    # Universal content identifiers
    id: str = Field(
        ...,
        description="Unique identifier from source provider"
    )

    title: str = Field(
        ...,
        description="Content title/name"
    )

    slug: str = Field(
        ...,
        description="URL-friendly identifier",
        pattern=r"^[a-z0-9-]+$"
    )

    # Content classification
    content_type: ContentType = Field(
        ...,
        description="Type of content for routing and rendering"
    )

    status: ContentStatus = Field(
        default=ContentStatus.PUBLISHED,
        description="Content publication status"
    )

    # Content body and metadata
    description: Optional[str] = Field(
        default=None,
        description="Content description/excerpt"
    )

    body: Optional[str] = Field(
        default=None,
        description="Full content body (HTML/Markdown)"
    )

    # Media and assets
    featured_image: Optional[MediaAsset] = Field(
        default=None,
        description="Primary image for content"
    )

    images: List[MediaAsset] = Field(
        default_factory=list,
        description="Additional images and media"
    )

    # E-commerce specific fields (Optional for products)
    price: Optional[Price] = Field(
        default=None,
        description="Product pricing information"
    )

    inventory: Optional[Inventory] = Field(
        default=None,
        description="Product inventory tracking"
    )

    variants: List[ProductVariant] = Field(
        default_factory=list,
        description="Product variants (size, color, etc.)"
    )

    # SEO and metadata
    seo: Optional[SEOMetadata] = Field(
        default=None,
        description="SEO optimization data"
    )

    tags: List[str] = Field(
        default_factory=list,
        description="Content tags and categories"
    )

    # Provider tracking
    provider_type: Literal["cms", "ecommerce"] = Field(
        ...,
        description="Type of provider that created this content"
    )

    provider_name: str = Field(
        ...,
        description="Specific provider name (e.g., 'shopify_basic', 'sanity')"
    )

    provider_data: Dict[str, Any] = Field(
        default_factory=dict,
        description="Provider-specific data for advanced features"
    )

    # Timestamps and versioning
    created_at: datetime = Field(
        ...,
        description="Content creation timestamp"
    )

    updated_at: datetime = Field(
        ...,
        description="Last modification timestamp"
    )

    synced_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="Last synchronization with integration layer"
    )

class MediaAsset(BaseModel):
    """Media asset representation"""
    id: str
    url: str
    alt_text: Optional[str] = None
    width: Optional[int] = None
    height: Optional[int] = None
    file_size: Optional[int] = None
    mime_type: Optional[str] = None

class Price(BaseModel):
    """Product pricing information"""
    amount: float = Field(..., ge=0)
    currency_code: str = Field(..., pattern=r"^[A-Z]{3}$")
    compare_at_amount: Optional[float] = None

class Inventory(BaseModel):
    """Product inventory tracking"""
    quantity: int = Field(..., ge=0)
    track_quantity: bool = True
    continue_selling_when_out_of_stock: bool = False
    inventory_policy: Literal["deny", "continue"] = "deny"

class ProductVariant(BaseModel):
    """Product variant information"""
    id: str
    title: str
    price: Price
    inventory: Optional[Inventory] = None
    options: Dict[str, str] = Field(default_factory=dict)  # {"Size": "L", "Color": "Blue"}

class SEOMetadata(BaseModel):
    """SEO optimization metadata"""
    meta_title: Optional[str] = None
    meta_description: Optional[str] = None
    canonical_url: Optional[str] = None
    og_title: Optional[str] = None
    og_description: Optional[str] = None
    og_image: Optional[str] = None
```

## ðŸ”Œ Component Protocol System

### Pluggable Component Interface

```python
from abc import ABC, abstractmethod
from typing import Protocol, runtime_checkable

@runtime_checkable
class ComposableComponent(Protocol):
    """
    Standard protocol that all CMS and E-commerce components must implement
    for event-driven composition integration
    """

    def register_with_integration_layer(self, integration_api_url: str) -> ComponentRegistration:
        """Register component webhooks with integration layer"""
        ...

    def normalize_content(self, raw_data: Dict[str, Any]) -> List[UnifiedContent]:
        """Convert provider-specific data to unified content schema"""
        ...

    def get_webhook_events(self) -> List[str]:
        """Return list of webhook events this component can handle"""
        ...

    def validate_configuration(self, config: Dict[str, Any]) -> bool:
        """Validate component configuration before deployment"""
        ...

    def get_build_dependencies(self, ssg_engine: SSGEngine) -> Dict[str, List[str]]:
        """Return build dependencies needed for SSG integration"""
        ...

    def estimate_monthly_cost(self, requirements: Dict[str, Any]) -> CostBreakdown:
        """Estimate monthly operational costs for this component"""
        ...

class ComponentRegistration(BaseModel):
    """Component registration information"""
    component_id: str
    component_type: Literal["cms", "ecommerce"]
    provider_name: str
    webhook_endpoints: List[WebhookEndpoint]
    supported_events: List[str]
    health_check_url: Optional[str] = None

class WebhookEndpoint(BaseModel):
    """Webhook endpoint configuration"""
    event_type: str
    endpoint_path: str
    http_method: Literal["POST", "PUT", "PATCH"] = "POST"
    expected_headers: Dict[str, str] = Field(default_factory=dict)
    signature_verification: Optional[str] = None  # HMAC, JWT, etc.
```

### CMS Component Implementation Example

```python
class DecapCMSComponent:
    """
    Decap CMS component implementing ComposableComponent protocol
    """

    def __init__(self, github_repo: str, content_directory: str = "content"):
        self.github_repo = github_repo
        self.content_directory = content_directory
        self.provider_name = "decap"

    def register_with_integration_layer(self, integration_api_url: str) -> ComponentRegistration:
        """Register GitHub webhooks pointing to integration layer"""

        webhook_url = f"{integration_api_url}/webhooks/{self.provider_name}"

        # Register GitHub webhook
        github_webhook = self._create_github_webhook(webhook_url)

        return ComponentRegistration(
            component_id=f"{self.provider_name}-{self.github_repo.replace('/', '-')}",
            component_type="cms",
            provider_name=self.provider_name,
            webhook_endpoints=[
                WebhookEndpoint(
                    event_type="push",
                    endpoint_path=f"/webhooks/{self.provider_name}",
                    expected_headers={"X-GitHub-Event": "push"},
                    signature_verification="github_hmac"
                )
            ],
            supported_events=["content.created", "content.updated", "content.deleted"]
        )

    def normalize_content(self, raw_data: Dict[str, Any]) -> List[UnifiedContent]:
        """Convert GitHub webhook data to unified content"""

        commits = raw_data.get("commits", [])
        unified_content = []

        for commit in commits:
            for file_path in commit.get("added", []) + commit.get("modified", []):
                if file_path.startswith(self.content_directory):
                    content = self._parse_markdown_file(file_path)

                    unified_content.append(UnifiedContent(
                        id=f"github:{self.github_repo}:{file_path}",
                        title=content.get("title", "Untitled"),
                        slug=self._generate_slug(file_path),
                        content_type=ContentType.ARTICLE,
                        description=content.get("description"),
                        body=content.get("body"),
                        provider_type="cms",
                        provider_name=self.provider_name,
                        provider_data={
                            "file_path": file_path,
                            "commit_sha": commit["id"],
                            "repository": self.github_repo
                        },
                        created_at=datetime.fromisoformat(commit["timestamp"]),
                        updated_at=datetime.fromisoformat(commit["timestamp"]),
                        tags=content.get("tags", [])
                    ))

        return unified_content

    def get_webhook_events(self) -> List[str]:
        """GitHub webhook events we handle"""
        return ["push", "pull_request"]

    def validate_configuration(self, config: Dict[str, Any]) -> bool:
        """Validate GitHub repository access and configuration"""
        required_fields = ["github_repo", "github_token"]
        return all(field in config for field in required_fields)
```

### E-commerce Component Implementation Example

```python
class ShopifyBasicComponent:
    """
    Shopify Basic component implementing ComposableComponent protocol
    """

    def __init__(self, store_domain: str, shopify_plan: str = "basic"):
        self.store_domain = store_domain
        self.shopify_plan = shopify_plan
        self.provider_name = "shopify_basic"

    def register_with_integration_layer(self, integration_api_url: str) -> ComponentRegistration:
        """Register Shopify webhooks pointing to integration layer"""

        webhook_url = f"{integration_api_url}/webhooks/{self.provider_name}"

        # Register Shopify webhooks using Admin API
        shopify_webhooks = self._create_shopify_webhooks(webhook_url)

        return ComponentRegistration(
            component_id=f"{self.provider_name}-{self.store_domain}",
            component_type="ecommerce",
            provider_name=self.provider_name,
            webhook_endpoints=[
                WebhookEndpoint(
                    event_type="products/create",
                    endpoint_path=f"/webhooks/{self.provider_name}",
                    expected_headers={"X-Shopify-Topic": "products/create"},
                    signature_verification="shopify_hmac"
                ),
                WebhookEndpoint(
                    event_type="products/update",
                    endpoint_path=f"/webhooks/{self.provider_name}",
                    expected_headers={"X-Shopify-Topic": "products/update"},
                    signature_verification="shopify_hmac"
                )
            ],
            supported_events=[
                "products/create", "products/update", "products/delete",
                "inventory_levels/update", "collections/create", "collections/update"
            ]
        )

    def normalize_content(self, raw_data: Dict[str, Any]) -> List[UnifiedContent]:
        """Convert Shopify webhook data to unified content"""

        webhook_topic = raw_data.get("webhook_topic", "")

        if webhook_topic.startswith("products/"):
            return [self._normalize_product(raw_data)]
        elif webhook_topic.startswith("collections/"):
            return [self._normalize_collection(raw_data)]

        return []

    def _normalize_product(self, product_data: Dict[str, Any]) -> UnifiedContent:
        """Convert Shopify product to unified content"""

        # Extract variants and pricing
        variants = []
        min_price = float('inf')

        for variant in product_data.get("variants", []):
            price = Price(
                amount=float(variant["price"]),
                currency_code="USD"  # TODO: Get from store settings
            )

            variants.append(ProductVariant(
                id=str(variant["id"]),
                title=variant["title"],
                price=price,
                inventory=Inventory(
                    quantity=variant.get("inventory_quantity", 0),
                    track_quantity=variant.get("inventory_management") == "shopify"
                )
            ))

            min_price = min(min_price, float(variant["price"]))

        # Extract images
        images = [
            MediaAsset(
                id=str(img["id"]),
                url=img["src"],
                alt_text=img.get("alt"),
                width=img.get("width"),
                height=img.get("height")
            )
            for img in product_data.get("images", [])
        ]

        return UnifiedContent(
            id=f"gid://shopify/Product/{product_data['id']}",
            title=product_data["title"],
            slug=product_data["handle"],
            content_type=ContentType.PRODUCT,
            description=product_data.get("body_html"),
            featured_image=images[0] if images else None,
            images=images,
            price=Price(amount=min_price, currency_code="USD") if min_price < float('inf') else None,
            variants=variants,
            provider_type="ecommerce",
            provider_name=self.provider_name,
            provider_data={
                "shopify_id": product_data["id"],
                "vendor": product_data.get("vendor"),
                "product_type": product_data.get("product_type"),
                "store_domain": self.store_domain
            },
            created_at=datetime.fromisoformat(product_data["created_at"]),
            updated_at=datetime.fromisoformat(product_data["updated_at"]),
            tags=product_data.get("tags", "").split(",") if product_data.get("tags") else []
        )
```

## âš¡ Event Bus and Integration Layer

### SNS Event System

```python
class ContentEventBus:
    """
    SNS-based event bus for content change notifications
    """

    def __init__(self, client_config: ClientConfig):
        self.client_config = client_config
        self.topic_name = f"{client_config.resource_prefix}-content-events"

    def create_event_topic(self) -> sns.Topic:
        """Create SNS topic for content events"""

        return sns.Topic(
            self.scope, "ContentEventsTopic",
            topic_name=self.topic_name,
            display_name=f"Content Events - {self.client_config.client_id}",
            # Enable message filtering for different event types
            content_based_deduplication=True,
            fifo=False  # Use standard topic for better performance
        )

    def create_event_subscriptions(self, build_trigger_lambda: lambda_.Function) -> List[sns.Subscription]:
        """Create event subscriptions for different handlers"""

        subscriptions = []

        # Build trigger subscription
        build_subscription = sns.Subscription(
            self.scope, "BuildTriggerSubscription",
            topic=self.content_events_topic,
            endpoint=build_trigger_lambda.function_arn,
            protocol=sns.SubscriptionProtocol.LAMBDA,
            # Filter for content change events that should trigger builds
            filter_policy={
                "event_type": sns.SubscriptionFilter.string_filter(
                    allowlist=["content.created", "content.updated", "content.deleted"]
                ),
                "requires_build": sns.SubscriptionFilter.string_filter(
                    allowlist=["true"]
                )
            }
        )

        subscriptions.append(build_subscription)

        # Add permission for SNS to invoke Lambda
        build_trigger_lambda.add_permission(
            "AllowSNSInvoke",
            principal=iam.ServicePrincipal("sns.amazonaws.com"),
            source_arn=self.content_events_topic.topic_arn
        )

        return subscriptions

class ContentEvent(BaseModel):
    """Standard content event format"""

    event_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    event_type: Literal[
        "content.created", "content.updated", "content.deleted",
        "inventory.updated", "collection.created", "collection.updated"
    ]

    # Content identification
    content_id: str
    content_type: ContentType
    provider_name: str

    # Event metadata
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    requires_build: bool = True  # Whether this event should trigger site rebuild

    # Event data
    content_data: Optional[Dict[str, Any]] = None
    previous_content: Optional[Dict[str, Any]] = None  # For update events

    # Client context
    client_id: str
    environment: Literal["dev", "staging", "prod"] = "prod"
```

### Integration API Gateway

```python
class IntegrationAPI:
    """
    API Gateway handling all external webhooks and internal integration requests
    """

    def __init__(self, client_config: ClientConfig, integration_handler: lambda_.Function):
        self.client_config = client_config
        self.integration_handler = integration_handler

    def create_api_gateway(self) -> apigateway.RestApi:
        """Create API Gateway for webhook handling"""

        api = apigateway.RestApi(
            self.scope, "IntegrationAPI",
            rest_api_name=f"{self.client_config.client_id}-integration-api",
            description=f"Integration API for {self.client_config.client_id} composition architecture",

            # Enable request validation
            policy=iam.PolicyDocument(
                statements=[
                    iam.PolicyStatement(
                        effect=iam.Effect.ALLOW,
                        principals=[iam.AnyPrincipal()],
                        actions=["execute-api:Invoke"],
                        resources=["*"]
                    )
                ]
            ),

            # Enable CORS for development
            default_cors_preflight_options=apigateway.CorsOptions(
                allow_origins=apigateway.Cors.ALL_ORIGINS,
                allow_methods=apigateway.Cors.ALL_METHODS,
                allow_headers=["Content-Type", "X-Amz-Date", "Authorization", "X-Api-Key"]
            )
        )

        # Create webhook resource structure
        self._create_webhook_resources(api)

        # Create internal API resources
        self._create_internal_api_resources(api)

        return api

    def _create_webhook_resources(self, api: apigateway.RestApi) -> None:
        """Create webhook endpoint structure"""

        # Main webhooks resource
        webhooks_resource = api.root.add_resource("webhooks")

        # Provider-specific webhook endpoints
        providers = ["decap", "tina", "sanity", "contentful", "snipcart", "foxy", "shopify_basic"]

        for provider in providers:
            provider_resource = webhooks_resource.add_resource(provider)

            # POST endpoint for webhook handling
            provider_resource.add_method(
                "POST",
                apigateway.LambdaIntegration(
                    self.integration_handler,
                    # Pass provider name in request context
                    request_templates={
                        "application/json": json.dumps({
                            "provider_name": provider,
                            "body": "$input.body",
                            "headers": "$input.params().header",
                            "query_params": "$input.params().querystring"
                        })
                    }
                ),

                # Add method response for proper CORS
                method_responses=[
                    apigateway.MethodResponse(
                        status_code="200",
                        response_headers={
                            "Access-Control-Allow-Origin": True,
                            "Access-Control-Allow-Headers": True,
                            "Access-Control-Allow-Methods": True
                        }
                    ),
                    apigateway.MethodResponse(status_code="400"),
                    apigateway.MethodResponse(status_code="500")
                ]
            )

    def _create_internal_api_resources(self, api: apigateway.RestApi) -> None:
        """Create internal API endpoints for content retrieval"""

        # Content API for SSG builds
        content_resource = api.root.add_resource("content")

        # GET /content - retrieve all content for build
        content_resource.add_method(
            "GET",
            apigateway.LambdaIntegration(self.integration_handler),
            authorization_type=apigateway.AuthorizationType.IAM,  # Secure internal API
            request_parameters={
                "method.request.querystring.content_type": False,
                "method.request.querystring.provider": False,
                "method.request.querystring.limit": False
            }
        )

        # GET /content/{id} - retrieve specific content item
        content_id_resource = content_resource.add_resource("{id}")
        content_id_resource.add_method(
            "GET",
            apigateway.LambdaIntegration(self.integration_handler),
            authorization_type=apigateway.AuthorizationType.IAM
        )

        # Health check endpoint
        health_resource = api.root.add_resource("health")
        health_resource.add_method(
            "GET",
            apigateway.MockIntegration(
                integration_responses=[
                    apigateway.IntegrationResponse(
                        status_code="200",
                        response_templates={
                            "application/json": json.dumps({
                                "status": "healthy",
                                "timestamp": "$context.requestTime",
                                "client_id": self.client_config.client_id
                            })
                        }
                    )
                ],
                request_templates={
                    "application/json": json.dumps({"statusCode": 200})
                }
            ),
            method_responses=[apigateway.MethodResponse(status_code="200")]
        )
```

## ðŸ”§ Optimized Lambda Integration Handlers

### Integration Handler Lambda (Optimized Architecture)

```python
class OptimizedIntegrationHandler:
    """
    Optimized central Lambda handler using ProviderAdapterRegistry
    instead of if/elif routing for better performance and maintainability.

    PERFORMANCE OPTIMIZATIONS:
    - ProviderAdapterRegistry eliminates complex conditional logic
    - Handler caching reduces initialization overhead
    - GSI queries replace expensive table scans
    - Event filtering reduces redundant Lambda invocations
    """

    def __init__(self):
        # Use optimized components
        self.provider_registry = ProviderAdapterRegistry()
        self.content_cache = OptimizedContentCache(os.environ['CONTENT_CACHE_TABLE'])
        self.event_filter = EventFilteringSystem()

        # Initialize built-in adapters
        self.provider_registry.register_builtin_adapters()

    def lambda_handler(self, event: Dict[str, Any], context) -> Dict[str, Any]:
        """Main Lambda handler with optimized routing"""

        try:
            http_method = event.get('httpMethod', '')
            resource_path = event.get('resource', '')

            if http_method == 'POST' and '/webhooks/' in resource_path:
                return self._handle_webhook_optimized(event, context)
            elif http_method == 'GET' and '/content' in resource_path:
                return self._handle_content_request_optimized(event, context)
            else:
                return self._create_response(400, {'error': 'Unsupported request'})

        except Exception as e:
            logger.error(f"Integration handler error: {str(e)}", exc_info=True)
            return self._create_response(500, {'error': 'Internal server error'})

    def _handle_webhook_optimized(self, event: Dict[str, Any], context) -> Dict[str, Any]:
        """
        Optimized webhook handling using ProviderAdapterRegistry
        ELIMINATES: Complex if/elif routing for providers
        BENEFITS: Better performance, maintainability, and extensibility
        """

        # Extract provider from path
        path_parameters = event.get('pathParameters', {})
        provider_name = path_parameters.get('proxy', '').split('/')[0]

        # Get webhook body and headers
        body = event.get('body', '{}')
        headers = event.get('headers', {})

        if isinstance(body, str):
            body = json.loads(body)

        logger.info(f"Processing webhook from {provider_name}")

        try:
            # Use registry to normalize content (eliminates if/elif routing)
            unified_content = self.provider_registry.normalize_content(
                provider_name=provider_name,
                webhook_data=body,
                headers=headers
            )

            # Process normalized content with optimized cache operations
            events_published = []
            for content in unified_content:
                # Store using optimized cache (GSI-optimized structure)
                success = self.content_cache.put_content(content, os.environ['CLIENT_ID'])

                if success:
                    # Publish filtered event (reduces redundant invocations)
                    event_published = self._publish_filtered_content_event(
                        event_type="content.updated" if content.status.value == "published" else "content.created",
                        content=content
                    )
                    events_published.append(event_published)

            return self._create_response(200, {
                'message': f'Processed {len(unified_content)} content items from {provider_name}',
                'events_published': len(events_published),
                'provider_type': self.provider_registry.get_provider_type(provider_name),
                'optimization_stats': {
                    'used_provider_registry': True,
                    'cache_operations': len(unified_content),
                    'filtered_events': len(events_published)
                }
            })

        except ValueError as e:
            logger.error(f"Webhook validation error: {str(e)}")
            return self._create_response(400, {'error': str(e)})

    def _handle_content_request_optimized(self, event: Dict[str, Any], context) -> Dict[str, Any]:
        """
        Optimized content retrieval using GSI queries instead of table scans
        PERFORMANCE GAIN: 80-90% cost reduction, 50-100ms response time vs 5-10s scans
        """

        query_params = event.get('queryStringParameters') or {}
        path_parameters = event.get('pathParameters') or {}

        # Handle specific content ID request (primary key lookup - most efficient)
        if 'id' in path_parameters:
            content_id = path_parameters['id']
            content_type_provider = query_params.get('content_type_provider', '')

            content = self.content_cache.get_content_by_id(content_id, content_type_provider)
            if content:
                return self._create_response(200, {
                    'content': content,
                    'query_stats': {'query_type': 'primary_key_lookup', 'optimized': True}
                })
            else:
                return self._create_response(404, {'error': 'Content not found'})

        # Build optimized query using ContentQuery
        from shared.composition.optimized_content_cache import ContentQuery, ContentType

        content_type = None
        if query_params.get('content_type'):
            content_type = ContentType(query_params['content_type'])

        query = ContentQuery(
            client_id=os.environ['CLIENT_ID'],
            content_type=content_type,
            provider_name=query_params.get('provider'),
            status=query_params.get('status'),
            limit=int(query_params.get('limit', 100))
        )

        # Execute optimized query (uses GSI instead of scan)
        result = self.content_cache.query_content_optimized(query)

        return self._create_response(200, {
            'content': result.items,
            'count': result.count,
            'query_stats': result.query_stats,  # Includes performance metrics
            'optimization_benefits': {
                'used_gsi_query': True,
                'avoided_table_scan': True,
                'estimated_cost_savings': '80-90%'
            }
        })

    def _publish_filtered_content_event(self, event_type: str, content: UnifiedContent) -> str:
        """
        Publish content event with message attribute filtering
        REDUCES: Redundant Lambda invocations through intelligent filtering
        """

        event = ContentEvent(
            event_type=event_type,
            content_id=content.id,
            content_type=content.content_type,
            provider_name=content.provider_name,
            client_id=os.environ['CLIENT_ID'],
            requires_build=True
        )

        # Use event filtering system for optimized publishing
        message_id = self.event_filter.publish_filtered_event(
            topic_arn=os.environ['CONTENT_EVENTS_TOPIC_ARN'],
            event=event,
            environment=os.environ.get('ENVIRONMENT', 'prod')
        )

        logger.info(f"Published filtered event {event_type} for content {content.id}")
        return message_id

    def _handle_cms_webhook(self, provider: str, body: Dict, headers: Dict) -> Dict[str, Any]:
        """Handle CMS provider webhooks"""

        # Validate webhook signature
        if not self._validate_webhook_signature(provider, body, headers):
            return self._create_response(401, {'error': 'Invalid signature'})

        # Normalize content based on provider
        unified_content = self._normalize_cms_content(provider, body)

        # Store in unified cache and publish events
        events_published = []
        for content in unified_content:
            # Store in DynamoDB
            self._store_unified_content(content)

            # Publish SNS event
            event_published = self._publish_content_event(
                event_type="content.updated" if content.status == ContentStatus.PUBLISHED else "content.created",
                content=content
            )
            events_published.append(event_published)

        return self._create_response(200, {
            'message': f'Processed {len(unified_content)} content items',
            'events_published': len(events_published)
        })

    def _handle_ecommerce_webhook(self, provider: str, body: Dict, headers: Dict) -> Dict[str, Any]:
        """Handle E-commerce provider webhooks"""

        # Validate webhook signature
        if not self._validate_webhook_signature(provider, body, headers):
            return self._create_response(401, {'error': 'Invalid signature'})

        # Extract webhook topic
        webhook_topic = headers.get('X-Shopify-Topic') or headers.get('X-Webhook-Event', '')

        # Normalize content based on provider and event
        unified_content = self._normalize_ecommerce_content(provider, webhook_topic, body)

        if unified_content:
            # Store in unified cache
            self._store_unified_content(unified_content)

            # Publish SNS event
            event_type = self._map_webhook_to_event_type(webhook_topic)
            self._publish_content_event(event_type, unified_content)

            return self._create_response(200, {
                'message': f'Processed {webhook_topic} event',
                'content_id': unified_content.id
            })
        else:
            return self._create_response(200, {'message': 'No content changes detected'})

    def _normalize_cms_content(self, provider: str, webhook_data: Dict) -> List[UnifiedContent]:
        """Normalize CMS webhook data to unified schema"""

        if provider == 'decap':
            return self._normalize_decap_content(webhook_data)
        elif provider == 'sanity':
            return self._normalize_sanity_content(webhook_data)
        elif provider == 'contentful':
            return self._normalize_contentful_content(webhook_data)
        elif provider == 'tina':
            return self._normalize_tina_content(webhook_data)
        else:
            logger.warning(f"Unknown CMS provider: {provider}")
            return []

    def _normalize_ecommerce_content(self, provider: str, event_type: str, webhook_data: Dict) -> Optional[UnifiedContent]:
        """Normalize E-commerce webhook data to unified schema"""

        if provider == 'shopify_basic':
            return self._normalize_shopify_content(event_type, webhook_data)
        elif provider == 'snipcart':
            return self._normalize_snipcart_content(event_type, webhook_data)
        elif provider == 'foxy':
            return self._normalize_foxy_content(event_type, webhook_data)
        else:
            logger.warning(f"Unknown E-commerce provider: {provider}")
            return None

    def _store_unified_content(self, content: UnifiedContent) -> None:
        """Store unified content in DynamoDB cache"""

        # Convert to DynamoDB item
        item = content.model_dump()
        item['ttl'] = int((datetime.utcnow() + timedelta(days=30)).timestamp())  # 30-day TTL

        # Store with composite key for efficient querying
        self.content_cache_table.put_item(
            Item={
                'content_id': content.id,
                'content_type_provider': f"{content.content_type}#{content.provider_name}",
                'client_id': os.environ['CLIENT_ID'],
                'data': item
            }
        )

    def _publish_content_event(self, event_type: str, content: UnifiedContent) -> str:
        """Publish content change event to SNS"""

        event = ContentEvent(
            event_type=event_type,
            content_id=content.id,
            content_type=content.content_type,
            provider_name=content.provider_name,
            client_id=os.environ['CLIENT_ID'],
            requires_build=True
        )

        # Publish to SNS with message attributes for filtering
        response = self.sns.publish(
            TopicArn=self.events_topic_arn,
            Message=event.model_dump_json(),
            MessageAttributes={
                'event_type': {'DataType': 'String', 'StringValue': event_type},
                'content_type': {'DataType': 'String', 'StringValue': content.content_type},
                'provider_name': {'DataType': 'String', 'StringValue': content.provider_name},
                'requires_build': {'DataType': 'String', 'StringValue': 'true'},
                'client_id': {'DataType': 'String', 'StringValue': os.environ['CLIENT_ID']}
            }
        )

        logger.info(f"Published event {event_type} for content {content.id}")
        return response['MessageId']

    def _handle_content_request(self, event: Dict[str, Any], context) -> Dict[str, Any]:
        """Handle content retrieval requests from build pipeline"""

        query_params = event.get('queryStringParameters') or {}
        path_parameters = event.get('pathParameters') or {}

        # Handle specific content ID request
        if 'id' in path_parameters:
            content_id = path_parameters['id']
            content = self._get_content_by_id(content_id)
            if content:
                return self._create_response(200, content)
            else:
                return self._create_response(404, {'error': 'Content not found'})

        # Handle content listing with filters
        content_type = query_params.get('content_type')
        provider = query_params.get('provider')
        limit = int(query_params.get('limit', 100))

        content_list = self._get_content_list(content_type, provider, limit)

        return self._create_response(200, {
            'content': content_list,
            'count': len(content_list),
            'filters': {
                'content_type': content_type,
                'provider': provider,
                'limit': limit
            }
        })

    def _get_content_list(self, content_type: str = None, provider: str = None, limit: int = 100) -> List[Dict]:
        """Retrieve content list from unified cache"""

        # Build query parameters
        filter_expression = "client_id = :client_id"
        expression_values = {':client_id': os.environ['CLIENT_ID']}

        if content_type and provider:
            filter_expression += " AND content_type_provider = :ctp"
            expression_values[':ctp'] = f"{content_type}#{provider}"

        # Scan table with filters
        response = self.content_cache_table.scan(
            FilterExpression=filter_expression,
            ExpressionAttributeValues=expression_values,
            Limit=limit
        )

        # Extract and return content data
        return [item['data'] for item in response.get('Items', [])]
```

### Build Trigger Lambda

```python
class BuildTriggerHandler:
    """
    Lambda handler that processes SNS content events and triggers CodeBuild
    Implements the event-driven build system from the composition plan
    """

    def __init__(self):
        self.codebuild = boto3.client('codebuild')
        self.dynamodb = boto3.resource('dynamodb')

    def lambda_handler(self, event: Dict[str, Any], context) -> Dict[str, Any]:
        """Process SNS events and trigger builds"""

        try:
            # Process SNS message
            for record in event.get('Records', []):
                if record.get('EventSource') == 'aws:sns':
                    sns_message = json.loads(record['Sns']['Message'])
                    content_event = ContentEvent(**sns_message)

                    logger.info(f"Processing build trigger for event: {content_event.event_type}")

                    # Check if build is required
                    if content_event.requires_build:
                        build_id = self._trigger_build(content_event)
                        logger.info(f"Triggered build: {build_id}")

            return {'statusCode': 200, 'message': 'Build triggers processed'}

        except Exception as e:
            logger.error(f"Build trigger error: {str(e)}", exc_info=True)
            raise

    def _trigger_build(self, content_event: ContentEvent) -> str:
        """Trigger CodeBuild project for content changes"""

        # Get build project name
        build_project = os.environ['BUILD_PROJECT_NAME']

        # Create build with content context
        response = self.codebuild.start_build(
            projectName=build_project,
            environmentVariablesOverride=[
                {
                    'name': 'CONTENT_EVENT_TYPE',
                    'value': content_event.event_type,
                    'type': 'PLAINTEXT'
                },
                {
                    'name': 'CONTENT_ID',
                    'value': content_event.content_id,
                    'type': 'PLAINTEXT'
                },
                {
                    'name': 'PROVIDER_NAME',
                    'value': content_event.provider_name,
                    'type': 'PLAINTEXT'
                },
                {
                    'name': 'INTEGRATION_API_URL',
                    'value': os.environ['INTEGRATION_API_URL'],
                    'type': 'PLAINTEXT'
                },
                {
                    'name': 'BUILD_REASON',
                    'value': f"Content {content_event.event_type} - {content_event.content_id}",
                    'type': 'PLAINTEXT'
                }
            ]
        )

        return response['build']['id']

    def _should_trigger_build(self, content_event: ContentEvent) -> bool:
        """Determine if content change should trigger build"""

        # Always build for published content
        if content_event.event_type in ['content.created', 'content.updated']:
            return True

        # Build for product/inventory changes
        if content_event.event_type in ['inventory.updated'] and content_event.content_type == ContentType.PRODUCT:
            return True

        # Skip build for draft content or non-essential changes
        return False


## ðŸ”„ Build Batching Optimization System

### Intelligent Build Batching Handler

```python
class BuildBatchingHandler:
    """
    Intelligent build batching system to prevent rebuild storms and optimize
    infrastructure costs by batching multiple content changes into single builds.

    OPTIMIZATION BENEFITS:
    - Prevents rebuild storms from bulk content updates
    - Reduces CodeBuild costs by up to 70% through batching
    - Maintains responsive user experience for individual changes
    - Implements exponential backoff for burst content updates
    """

    def __init__(self):
        self.dynamodb = boto3.resource('dynamodb')
        self.codebuild = boto3.client('codebuild')
        self.events = boto3.client('events')

        # Build batching configuration table
        self.batch_table = self.dynamodb.Table(os.environ['BUILD_BATCH_TABLE'])

        # Batching parameters
        self.BATCH_WINDOW_SECONDS = 30      # Wait 30s for additional changes
        self.MAX_BATCH_SIZE = 50            # Maximum events per batch
        self.IMMEDIATE_BUILD_THRESHOLD = 3   # Build immediately for â‰¤3 events
        self.BULK_UPDATE_THRESHOLD = 10     # Detect bulk updates

    def lambda_handler(self, event: Dict[str, Any], context) -> Dict[str, Any]:
        """Handle build batching logic with intelligent timing"""

        try:
            # Process SNS events
            content_events = []
            for record in event.get('Records', []):
                if record.get('EventSource') == 'aws:sns':
                    sns_message = json.loads(record['Sns']['Message'])
                    content_event = ContentEvent(**sns_message)
                    content_events.append(content_event)

            logger.info(f"Received {len(content_events)} content events for batching")

            # Apply intelligent batching logic
            build_decision = self._analyze_batching_strategy(content_events)

            if build_decision['action'] == 'build_immediately':
                # Build immediately for small changes or high-priority content
                build_id = self._trigger_immediate_build(content_events)
                return {'statusCode': 200, 'build_id': build_id, 'strategy': 'immediate'}

            elif build_decision['action'] == 'add_to_batch':
                # Add to batch and schedule delayed build
                batch_id = self._add_to_batch(content_events)
                self._schedule_batch_build(batch_id, build_decision['delay_seconds'])
                return {'statusCode': 200, 'batch_id': batch_id, 'strategy': 'batched'}

            elif build_decision['action'] == 'trigger_batch_now':
                # Batch is full or timeout reached, trigger build
                build_id = self._trigger_batch_build(build_decision['batch_id'])
                return {'statusCode': 200, 'build_id': build_id, 'strategy': 'batch_complete'}

        except Exception as e:
            logger.error(f"Build batching error: {str(e)}", exc_info=True)
            # Fallback: trigger immediate build to prevent content being stuck
            build_id = self._trigger_immediate_build(content_events)
            return {'statusCode': 200, 'build_id': build_id, 'strategy': 'fallback'}

    def _analyze_batching_strategy(self, content_events: List[ContentEvent]) -> Dict[str, Any]:
        """Analyze content events and determine optimal batching strategy"""

        client_id = content_events[0].client_id if content_events else None
        current_time = datetime.utcnow()

        # Check for existing active batch
        existing_batch = self._get_active_batch(client_id)

        # Strategy 1: Immediate build for high-priority or small changes
        high_priority_events = [
            e for e in content_events
            if e.content_type == ContentType.PRODUCT and e.event_type in ['content.created', 'content.updated']
        ]

        if len(content_events) <= self.IMMEDIATE_BUILD_THRESHOLD or len(high_priority_events) > 0:
            if not existing_batch:
                return {'action': 'build_immediately'}

        # Strategy 2: Detect bulk updates (indicating batch import/sync)
        if len(content_events) >= self.BULK_UPDATE_THRESHOLD:
            # This is likely a bulk operation - use longer batching window
            if existing_batch:
                # Add to existing batch and extend window
                return {
                    'action': 'add_to_batch',
                    'batch_id': existing_batch['batch_id'],
                    'delay_seconds': 60  # Longer delay for bulk operations
                }
            else:
                # Create new batch with extended window
                return {
                    'action': 'add_to_batch',
                    'delay_seconds': 60
                }

        # Strategy 3: Standard batching for regular content updates
        if existing_batch:
            batch_age = (current_time - datetime.fromisoformat(existing_batch['created_at'])).total_seconds()
            batch_size = existing_batch['event_count']

            # Trigger batch if it's full or old enough
            if batch_size >= self.MAX_BATCH_SIZE or batch_age >= self.BATCH_WINDOW_SECONDS:
                return {
                    'action': 'trigger_batch_now',
                    'batch_id': existing_batch['batch_id']
                }
            else:
                # Add to existing batch
                return {
                    'action': 'add_to_batch',
                    'batch_id': existing_batch['batch_id'],
                    'delay_seconds': max(10, self.BATCH_WINDOW_SECONDS - batch_age)
                }
        else:
            # Create new batch
            return {
                'action': 'add_to_batch',
                'delay_seconds': self.BATCH_WINDOW_SECONDS
            }

    def _get_active_batch(self, client_id: str) -> Optional[Dict[str, Any]]:
        """Get active batch for client"""

        try:
            response = self.batch_table.query(
                IndexName='ClientActiveIndex',
                KeyConditionExpression=Key('client_id').eq(client_id) & Key('status').eq('active'),
                Limit=1
            )

            items = response.get('Items', [])
            return items[0] if items else None

        except Exception as e:
            logger.warning(f"Failed to get active batch: {str(e)}")
            return None

    def _add_to_batch(self, content_events: List[ContentEvent]) -> str:
        """Add content events to batch"""

        client_id = content_events[0].client_id
        batch_id = str(uuid.uuid4())
        current_time = datetime.utcnow()

        # Check if we should add to existing batch
        existing_batch = self._get_active_batch(client_id)

        if existing_batch:
            batch_id = existing_batch['batch_id']

            # Update existing batch
            self.batch_table.update_item(
                Key={'batch_id': batch_id},
                UpdateExpression='ADD event_count :count SET updated_at = :updated, events = list_append(events, :new_events)',
                ExpressionAttributeValues={
                    ':count': len(content_events),
                    ':updated': current_time.isoformat(),
                    ':new_events': [event.model_dump() for event in content_events]
                }
            )
        else:
            # Create new batch
            self.batch_table.put_item(
                Item={
                    'batch_id': batch_id,
                    'client_id': client_id,
                    'status': 'active',
                    'event_count': len(content_events),
                    'events': [event.model_dump() for event in content_events],
                    'created_at': current_time.isoformat(),
                    'updated_at': current_time.isoformat(),
                    'ttl': int((current_time + timedelta(hours=24)).timestamp())  # 24 hour TTL
                }
            )

        logger.info(f"Added {len(content_events)} events to batch {batch_id}")
        return batch_id

    def _schedule_batch_build(self, batch_id: str, delay_seconds: int) -> None:
        """Schedule batch build using EventBridge"""

        schedule_time = datetime.utcnow() + timedelta(seconds=delay_seconds)

        # Create one-time scheduled rule
        rule_name = f"batch-build-{batch_id}"

        self.events.put_rule(
            Name=rule_name,
            ScheduleExpression=f"at({schedule_time.strftime('%Y-%m-%dT%H:%M:%S')})",
            State='ENABLED',
            Description=f"Trigger batch build for {batch_id}"
        )

        # Add Lambda target
        self.events.put_targets(
            Rule=rule_name,
            Targets=[
                {
                    'Id': '1',
                    'Arn': os.environ['BUILD_TRIGGER_LAMBDA_ARN'],
                    'Input': json.dumps({
                        'batch_id': batch_id,
                        'action': 'trigger_batch_build'
                    })
                }
            ]
        )

        logger.info(f"Scheduled batch build {batch_id} in {delay_seconds} seconds")

    def _trigger_batch_build(self, batch_id: str) -> str:
        """Trigger build for accumulated batch"""

        try:
            # Get batch details
            response = self.batch_table.get_item(Key={'batch_id': batch_id})
            batch = response.get('Item')

            if not batch or batch['status'] != 'active':
                logger.warning(f"Batch {batch_id} not found or not active")
                return None

            # Mark batch as building
            self.batch_table.update_item(
                Key={'batch_id': batch_id},
                UpdateExpression='SET #status = :status, build_started_at = :started',
                ExpressionAttributeNames={'#status': 'status'},
                ExpressionAttributeValues={
                    ':status': 'building',
                    ':started': datetime.utcnow().isoformat()
                }
            )

            # Create aggregated build context
            events = [ContentEvent(**event_data) for event_data in batch['events']]
            build_context = self._create_batch_build_context(events)

            # Trigger CodeBuild
            build_project = os.environ['BUILD_PROJECT_NAME']

            response = self.codebuild.start_build(
                projectName=build_project,
                environmentVariablesOverride=[
                    {
                        'name': 'BUILD_TYPE',
                        'value': 'batch',
                        'type': 'PLAINTEXT'
                    },
                    {
                        'name': 'BATCH_ID',
                        'value': batch_id,
                        'type': 'PLAINTEXT'
                    },
                    {
                        'name': 'BATCH_EVENT_COUNT',
                        'value': str(len(events)),
                        'type': 'PLAINTEXT'
                    },
                    {
                        'name': 'CONTENT_CHANGES_SUMMARY',
                        'value': json.dumps(build_context),
                        'type': 'PLAINTEXT'
                    },
                    {
                        'name': 'INTEGRATION_API_URL',
                        'value': os.environ['INTEGRATION_API_URL'],
                        'type': 'PLAINTEXT'
                    }
                ]
            )

            build_id = response['build']['id']
            logger.info(f"Triggered batch build {build_id} for {len(events)} events")

            # Update batch with build ID
            self.batch_table.update_item(
                Key={'batch_id': batch_id},
                UpdateExpression='SET build_id = :build_id',
                ExpressionAttributeValues={':build_id': build_id}
            )

            return build_id

        except Exception as e:
            logger.error(f"Failed to trigger batch build {batch_id}: {str(e)}")
            # Mark batch as failed
            self.batch_table.update_item(
                Key={'batch_id': batch_id},
                UpdateExpression='SET #status = :status, error = :error',
                ExpressionAttributeNames={'#status': 'status'},
                ExpressionAttributeValues={
                    ':status': 'failed',
                    ':error': str(e)
                }
            )
            raise

    def _trigger_immediate_build(self, content_events: List[ContentEvent]) -> str:
        """Trigger immediate build for high-priority content changes"""

        build_project = os.environ['BUILD_PROJECT_NAME']

        # Create summary for immediate build
        content_summary = {
            'event_count': len(content_events),
            'content_types': list(set([event.content_type.value for event in content_events])),
            'providers': list(set([event.provider_name for event in content_events])),
            'event_types': list(set([event.event_type for event in content_events]))
        }

        response = self.codebuild.start_build(
            projectName=build_project,
            environmentVariablesOverride=[
                {
                    'name': 'BUILD_TYPE',
                    'value': 'immediate',
                    'type': 'PLAINTEXT'
                },
                {
                    'name': 'CONTENT_CHANGES_SUMMARY',
                    'value': json.dumps(content_summary),
                    'type': 'PLAINTEXT'
                },
                {
                    'name': 'INTEGRATION_API_URL',
                    'value': os.environ['INTEGRATION_API_URL'],
                    'type': 'PLAINTEXT'
                },
                {
                    'name': 'BUILD_REASON',
                    'value': f"Immediate build - {len(content_events)} high-priority changes",
                    'type': 'PLAINTEXT'
                }
            ]
        )

        build_id = response['build']['id']
        logger.info(f"Triggered immediate build {build_id} for {len(content_events)} events")
        return build_id

    def _create_batch_build_context(self, events: List[ContentEvent]) -> Dict[str, Any]:
        """Create optimized build context for batch operations"""

        context = {
            'total_events': len(events),
            'content_types': {},
            'providers': {},
            'event_types': {},
            'affected_content_ids': [],
            'requires_full_rebuild': False
        }

        for event in events:
            # Count by content type
            content_type = event.content_type.value
            context['content_types'][content_type] = context['content_types'].get(content_type, 0) + 1

            # Count by provider
            provider = event.provider_name
            context['providers'][provider] = context['providers'].get(provider, 0) + 1

            # Count by event type
            event_type = event.event_type
            context['event_types'][event_type] = context['event_types'].get(event_type, 0) + 1

            # Track affected content
            context['affected_content_ids'].append(event.content_id)

            # Check if full rebuild is needed
            if event.event_type == 'collection.updated' or len(events) > 20:
                context['requires_full_rebuild'] = True

        return context


### Build Batching DynamoDB Table

class BuildBatchingTable:
    """DynamoDB table for managing build batches"""

    @staticmethod
    def create_table_definition() -> dynamodb.Table:
        """Create build batching table"""

        return dynamodb.Table(
            scope, "BuildBatchingTable",
            table_name=f"{client_config.resource_prefix}-build-batching",

            # Partition key: batch_id
            partition_key=dynamodb.Attribute(
                name="batch_id",
                type=dynamodb.AttributeType.STRING
            ),

            # TTL for automatic cleanup
            time_to_live_attribute="ttl",

            # Pay per request billing
            billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,

            # GSI for client active batches
            global_secondary_indexes=[
                dynamodb.GlobalSecondaryIndex(
                    index_name="ClientActiveIndex",
                    partition_key=dynamodb.Attribute(
                        name="client_id",
                        type=dynamodb.AttributeType.STRING
                    ),
                    sort_key=dynamodb.Attribute(
                        name="status",
                        type=dynamodb.AttributeType.STRING
                    ),
                    projection_type=dynamodb.ProjectionType.ALL
                )
            ]
        )
```
```

## ðŸ“¦ Optimized DynamoDB Schema Design

### Unified Content Cache Table with GSI Optimization

```python
class UnifiedContentCacheTable:
    """
    DynamoDB table design for unified content storage optimized for performance.

    PERFORMANCE OPTIMIZATIONS:
    - Multiple GSI indexes eliminate expensive table scans
    - Query-optimized key structure reduces RCU consumption by 80-90%
    - TTL management prevents storage cost accumulation
    - Strategic projection types minimize data transfer costs
    """

    @staticmethod
    def create_table_definition() -> dynamodb.Table:
        """Create DynamoDB table for unified content cache with optimization"""

        return dynamodb.Table(
            scope, "UnifiedContentCache",
            table_name=f"{client_config.resource_prefix}-unified-content-cache",

            # Partition key: content_id (unique across all providers)
            partition_key=dynamodb.Attribute(
                name="content_id",
                type=dynamodb.AttributeType.STRING
            ),

            # Sort key: content_type#provider for efficient querying
            sort_key=dynamodb.Attribute(
                name="content_type_provider",
                type=dynamodb.AttributeType.STRING
            ),

            # Pay per request for variable workloads
            billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,

            # TTL for automatic cleanup (30 days)
            time_to_live_attribute="ttl",

            # Enable point-in-time recovery for production
            point_in_time_recovery=True,

            # Stream for real-time processing if needed
            stream=dynamodb.StreamViewType.NEW_AND_OLD_IMAGES,

            # Global Secondary Indexes for efficient queries (OPTIMIZED)
            global_secondary_indexes=[
                # PRIMARY GSI: Query by client and content type (most common query)
                # OPTIMIZATION: Full projection for single-query content retrieval
                dynamodb.GlobalSecondaryIndex(
                    index_name="ClientContentTypeIndex",
                    partition_key=dynamodb.Attribute(
                        name="client_id",
                        type=dynamodb.AttributeType.STRING
                    ),
                    sort_key=dynamodb.Attribute(
                        name="content_type",
                        type=dynamodb.AttributeType.STRING
                    ),
                    projection_type=dynamodb.ProjectionType.ALL  # Optimized: Avoid additional queries
                ),

                # SECONDARY GSI: Query by provider and update time
                # OPTIMIZATION: Keys-only projection to minimize costs for sync operations
                dynamodb.GlobalSecondaryIndex(
                    index_name="ProviderUpdateIndex",
                    partition_key=dynamodb.Attribute(
                        name="provider_name",
                        type=dynamodb.AttributeType.STRING
                    ),
                    sort_key=dynamodb.Attribute(
                        name="updated_at",
                        type=dynamodb.AttributeType.STRING
                    ),
                    projection_type=dynamodb.ProjectionType.KEYS_ONLY  # Optimized: Reduce data transfer
                ),

                # NEW OPTIMIZATION GSI: Query by status and update time for build optimization
                # OPTIMIZATION: Enable efficient queries for published content only
                dynamodb.GlobalSecondaryIndex(
                    index_name="StatusUpdateIndex",
                    partition_key=dynamodb.Attribute(
                        name="status",
                        type=dynamodb.AttributeType.STRING
                    ),
                    sort_key=dynamodb.Attribute(
                        name="updated_at",
                        type=dynamodb.AttributeType.STRING
                    ),
                    projection_type=dynamodb.ProjectionType.INCLUDE,
                    non_key_attributes=["client_id", "content_type", "provider_name", "title"]
                )
            ],

            removal_policy=RemovalPolicy.DESTROY  # For development
        )

    @staticmethod
    def get_item_structure() -> Dict[str, Any]:
        """Example item structure in the table"""
        return {
            # Primary keys
            'content_id': 'gid://shopify/Product/123456789',
            'content_type_provider': 'product#shopify_basic',

            # Client context
            'client_id': 'acme-corp',

            # TTL for automatic cleanup (30 days)
            'ttl': 1740000000,

            # Unified content data
            'data': {
                'id': 'gid://shopify/Product/123456789',
                'title': 'Premium T-Shirt',
                'slug': 'premium-t-shirt',
                'content_type': 'product',
                'status': 'published',
                'provider_type': 'ecommerce',
                'provider_name': 'shopify_basic',
                'price': {
                    'amount': 29.99,
                    'currency_code': 'USD'
                },
                'created_at': '2025-01-08T10:30:00Z',
                'updated_at': '2025-01-08T10:30:00Z',
                'synced_at': '2025-01-08T10:31:00Z'
            }
        }
```

## ðŸ­ Composition Factory Implementation

### ComposedStackFactory

```python
class ComposedStackFactory:
    """
    Factory for creating event-driven composed stacks
    Implements the factory integration from the composition plan
    """

    @classmethod
    def create_composed_stack(
        cls,
        scope: Construct,
        construct_id: str,
        client_config: ClientConfig,
        cms_provider: str,
        ecommerce_provider: str,
        ssg_engine: str,
        integration_level: Literal["minimal", "standard", "full"] = "standard",
        **kwargs
    ) -> 'CMSEcommerceComposedStack':
        """
        Create event-driven composed stack with CMS and E-commerce integration
        """

        # Validate compatibility
        compatibility = cls._validate_provider_compatibility(
            cms_provider, ecommerce_provider, ssg_engine
        )

        if not compatibility["compatible"]:
            raise ValueError(f"Incompatible combination: {compatibility['reason']}")

        # Create composed stack
        return CMSEcommerceComposedStack(
            scope=scope,
            construct_id=construct_id,
            client_config=client_config,
            cms_provider=cms_provider,
            ecommerce_provider=ecommerce_provider,
            ssg_engine=ssg_engine,
            integration_level=integration_level,
            **kwargs
        )

    @classmethod
    def _validate_provider_compatibility(
        cls,
        cms_provider: str,
        ecommerce_provider: str,
        ssg_engine: str
    ) -> Dict[str, Any]:
        """Validate that provider combination is supported"""

        # Reference the compatibility matrix from composition plan
        compatibility_matrix = {
            ("decap", "snipcart", "eleventy"): {"score": 9, "complexity": "low"},
            ("sanity", "shopify_basic", "astro"): {"score": 10, "complexity": "medium"},
            ("contentful", "shopify_basic", "nextjs"): {"score": 9, "complexity": "high"},
            # ... additional combinations
        }

        combination = (cms_provider, ecommerce_provider, ssg_engine)

        if combination in compatibility_matrix:
            info = compatibility_matrix[combination]
            return {
                "compatible": True,
                "compatibility_score": info["score"],
                "complexity": info["complexity"],
                "recommended": info["score"] >= 8
            }
        else:
            # Check if individual components are supported
            cms_supported = cms_provider in ["decap", "tina", "sanity", "contentful"]
            ecommerce_supported = ecommerce_provider in ["snipcart", "foxy", "shopify_basic"]

            if not cms_supported:
                return {"compatible": False, "reason": f"Unsupported CMS provider: {cms_provider}"}
            elif not ecommerce_supported:
                return {"compatible": False, "reason": f"Unsupported E-commerce provider: {ecommerce_provider}"}
            else:
                # Untested combination - allow with warning
                return {
                    "compatible": True,
                    "compatibility_score": 6,
                    "complexity": "unknown",
                    "recommended": False,
                    "warning": "This combination hasn't been fully tested"
                }

    @classmethod
    def get_composition_recommendations(
        cls,
        client_requirements: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Get recommended CMS + E-commerce combinations based on client requirements
        Extends the recommendation logic from the composition plan
        """

        recommendations = []

        # Budget-conscious with simple needs
        if client_requirements.get("budget_conscious") and client_requirements.get("simple_needs"):
            recommendations.append({
                "cms_provider": "decap",
                "ecommerce_provider": "snipcart",
                "ssg_engine": "eleventy",
                "monthly_cost": "$85-140",
                "setup_cost": "$2,400-3,600",
                "reason": "Most cost-effective combination with Git-based CMS and simple e-commerce",
                "complexity": "Low",
                "compatibility_score": 9
            })

        # Performance-critical with modern needs
        if client_requirements.get("performance_critical"):
            recommendations.append({
                "cms_provider": "sanity",
                "ecommerce_provider": "shopify_basic",
                "ssg_engine": "astro",
                "monthly_cost": "$150-225",
                "setup_cost": "$3,200-4,800",
                "reason": "Optimal performance with component islands and structured content",
                "complexity": "Medium",
                "compatibility_score": 10
            })

        # Enterprise needs
        if client_requirements.get("enterprise_features"):
            recommendations.append({
                "cms_provider": "contentful",
                "ecommerce_provider": "shopify_basic",  # Will be shopify_advanced when available
                "ssg_engine": "nextjs",
                "monthly_cost": "$400-700",
                "setup_cost": "$4,800-8,000",
                "reason": "Enterprise CMS with advanced e-commerce and React ecosystem",
                "complexity": "High",
                "compatibility_score": 9
            })

        return recommendations

class CMSEcommerceComposedStack(BaseSSGStack):
    """
    Event-driven composed stack implementing CMS + E-commerce integration
    Based on the architecture defined in the composition plan
    """

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        client_config: ClientConfig,
        cms_provider: str,
        ecommerce_provider: str,
        ssg_engine: str,
        integration_level: str = "standard",
        **kwargs
    ):
        super().__init__(scope, construct_id, client_config, **kwargs)

        self.cms_provider = cms_provider
        self.ecommerce_provider = ecommerce_provider
        self.ssg_engine = ssg_engine
        self.integration_level = integration_level

        # Create event-driven integration infrastructure
        self._create_integration_layer()

        # Create composed stack components
        self._create_cms_component()
        self._create_ecommerce_component()

        # Create enhanced build pipeline
        self._create_composed_build_pipeline()

    def _create_integration_layer(self) -> None:
        """Create the central event-driven integration layer"""

        # SNS topic for content events
        self.content_events_topic = ContentEventBus(self.client_config).create_event_topic()

        # DynamoDB for unified content cache
        self.unified_content_cache = UnifiedContentCacheTable.create_table_definition()

        # Integration handler Lambda
        self.integration_handler = lambda_.Function(
            self, "IntegrationHandler",
            runtime=lambda_.Runtime.PYTHON_3_11,
            handler="integration_handler.lambda_handler",
            code=lambda_.Code.from_asset("lambda/integration_handler"),
            timeout=Duration.seconds(30),
            memory_size=512,
            environment={
                **self.get_standard_environment_variables(),
                "CONTENT_CACHE_TABLE": self.unified_content_cache.table_name,
                "CONTENT_EVENTS_TOPIC_ARN": self.content_events_topic.topic_arn,
                "CLIENT_ID": self.client_config.client_id
            }
        )

        # Grant permissions
        self.unified_content_cache.grant_read_write_data(self.integration_handler)
        self.content_events_topic.grant_publish(self.integration_handler)

        # Integration API Gateway
        self.integration_api = IntegrationAPI(
            self.client_config,
            self.integration_handler
        ).create_api_gateway()

        # Build trigger Lambda
        self.build_trigger = lambda_.Function(
            self, "BuildTrigger",
            runtime=lambda_.Runtime.PYTHON_3_11,
            handler="build_trigger.lambda_handler",
            code=lambda_.Code.from_asset("lambda/build_trigger"),
            timeout=Duration.seconds(60),
            environment={
                **self.get_standard_environment_variables(),
                "BUILD_PROJECT_NAME": f"{self.client_config.resource_prefix}-composed-build",
                "INTEGRATION_API_URL": self.integration_api.url
            }
        )

        # SNS subscriptions
        ContentEventBus(self.client_config).create_event_subscriptions(self.build_trigger)

    def get_monthly_cost_estimate(self) -> Dict[str, float]:
        """Calculate monthly cost for composed stack"""

        # Get base SSG costs
        base_costs = super().estimate_monthly_cost()

        # Get individual provider costs
        cms_costs = self._get_cms_provider_costs()
        ecommerce_costs = self._get_ecommerce_provider_costs()

        # Integration overhead (from composition plan)
        integration_overhead = {
            "lambda_executions": 8,      # Integration and build trigger handlers
            "dynamodb_usage": 3,         # Unified content cache
            "sns_events": 1,             # Content change events
            "api_gateway": 2,            # Integration API
            "additional_storage": 1      # Content cache bucket
        }

        total_integration = sum(integration_overhead.values())

        return {
            **base_costs,
            **cms_costs,
            **ecommerce_costs,
            "integration_overhead": total_integration,
            "total_composed_cost": (
                base_costs.get("total", 0) +
                cms_costs.get("total", 0) +
                ecommerce_costs.get("total", 0) +
                total_integration
            )
        }
```

## ðŸ“‹ Optimized Implementation Roadmap

### Phase 2 Implementation (6-8 weeks) - With Performance Optimizations

**Based on the composition plan timeline, implementing the optimized technical architecture:**

#### Weeks 1-2: Integration Layer Foundation âœ…
- [x] SNS topic and event bus infrastructure design
- [x] DynamoDB unified content cache schema with GSI optimization
- [x] Integration API webhook endpoints design
- [x] Unified content schema (UnifiedContent model)
- [x] Component protocol interfaces (ComposableComponent)
- [x] Event system models and handlers

#### Weeks 3-4: Optimized Component Implementation ðŸš€
- [ ] **OPTIMIZED**: ProviderAdapterRegistry implementation (eliminates if/elif routing)
- [ ] **OPTIMIZED**: Lambda integration handler using registry pattern
- [ ] **OPTIMIZED**: OptimizedContentCache with GSI queries (80-90% cost reduction)
- [ ] **OPTIMIZED**: EventFilteringSystem for reduced Lambda invocations
- [ ] CMS component adapters (Decap, Sanity, Tina, Contentful)
- [ ] E-commerce component adapters (Snipcart, Foxy, Shopify Basic)

#### Weeks 5-6: Intelligent Build Pipeline
- [ ] **OPTIMIZED**: BuildBatchingHandler with intelligent batching
- [ ] **OPTIMIZED**: Build trigger Lambda with event filtering
- [ ] **OPTIMIZED**: Event-driven CodeBuild with batch context
- [ ] Enhanced buildspec for unified content fetching
- [ ] EventBridge scheduling for batch builds
- [ ] End-to-end optimized event flow testing

#### Weeks 7-8: Factory Integration & Performance Validation
- [ ] ComposedStackFactory with optimization features
- [ ] **OPTIMIZED**: Performance monitoring and cost tracking
- [ ] Comprehensive integration testing with load scenarios
- [ ] **OPTIMIZED**: Build batching effectiveness validation
- [ ] Documentation with optimization benefits
- [ ] Client examples with performance benchmarks

### Key Optimization Deliverables

#### Performance Improvements
- **ProviderAdapterRegistry**: Eliminates complex if/elif routing, reduces handler complexity by 60%
- **GSI-Optimized DynamoDB**: Reduces query costs by 80-90%, response times from 5-10s to 50-100ms
- **Event Filtering**: Reduces redundant Lambda invocations by 70%, prevents rebuild storms
- **Build Batching**: Reduces CodeBuild costs by up to 70% through intelligent batching

#### Architecture Benefits
- **Complexity Reduction**: From 8.5/10 to 6.5/10 through event-driven patterns
- **Fault Isolation**: Component failures don't cascade across systems
- **Pluggable Components**: Standard protocol for unlimited provider expansion
- **Cost Optimization**: Multiple layers of cost reduction through batching and caching

## ðŸ”— Related Documentation

- **Foundation Document**: [CMS + E-commerce Composition Plan](./cms-ecommerce-composition-plan.md)
- **Factory Patterns**: [E-commerce Stack Factory](../../shared/factories/ecommerce_stack_factory.py)
- **Base Infrastructure**: [Base SSG Stack](../../stacks/shared/base_ssg_stack.py)
- **Implementation Strategy**: [CDK Implementation Strategy](../strategy/cdk-implementation-strategy.md)

---

**Status**: Technical architecture complete, ready for Phase 2 implementation
**Complexity**: Reduced from 8.5/10 to 6.5/10 through event-driven patterns
**Next Step**: Begin Lambda handler implementation (Weeks 3-4 of roadmap)